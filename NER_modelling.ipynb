{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5909af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f15384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ner_dataset.txt\", encoding= 'unicode_escape', sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "689f4c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c922728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100ddfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1           NaN             of   IN      O\n",
       "2           NaN  demonstrators  NNS      O\n",
       "3           NaN           have  VBP      O\n",
       "4           NaN        marched  VBN      O\n",
       "5           NaN        through   IN      O\n",
       "6           NaN         London  NNP  B-geo\n",
       "7           NaN             to   TO      O\n",
       "8           NaN        protest   VB      O\n",
       "9           NaN            the   DT      O\n",
       "10          NaN            war   NN      O\n",
       "11          NaN             in   IN      O\n",
       "12          NaN           Iraq  NNP  B-geo\n",
       "13          NaN            and   CC      O\n",
       "14          NaN         demand   VB      O\n",
       "15          NaN            the   DT      O\n",
       "16          NaN     withdrawal   NN      O\n",
       "17          NaN             of   IN      O\n",
       "18          NaN        British   JJ  B-gpe\n",
       "19          NaN         troops  NNS      O"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c7d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_dict_map(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['Tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok\n",
    "token2idx, idx2token = get_dict_map(data, 'token')\n",
    "tag2idx, idx2tag = get_dict_map(data, 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35ecc06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['Word'].map(token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252e45f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'petrochemicals': 0,\n",
       " 'rain-fed': 1,\n",
       " '1004': 2,\n",
       " 'dinosaurs': 3,\n",
       " 'Tyre': 4,\n",
       " 'traditional': 5,\n",
       " 'sow': 6,\n",
       " 'humor': 7,\n",
       " 'brings': 8,\n",
       " 'Weyn': 9,\n",
       " '1935': 10,\n",
       " 'prefer': 11,\n",
       " 'Otto': 12,\n",
       " 'rutile': 13,\n",
       " 'quotations': 14,\n",
       " 'IMF-World': 15,\n",
       " 'shared': 16,\n",
       " 'Jimenez': 17,\n",
       " 'three-story': 18,\n",
       " 'Sabeel': 19,\n",
       " 'enclaves': 20,\n",
       " 'leveraged': 21,\n",
       " 'cursed': 22,\n",
       " 'al-Qaim': 23,\n",
       " 'Unocal': 24,\n",
       " 'Juvenile': 25,\n",
       " 'Alastair': 26,\n",
       " 'SANA': 27,\n",
       " 'appropriately': 28,\n",
       " '70th': 29,\n",
       " 'spacewalk': 30,\n",
       " 'IEDs': 31,\n",
       " 're-structuring': 32,\n",
       " 'chains': 33,\n",
       " 'sponsors': 34,\n",
       " 'primaries': 35,\n",
       " 'Chookiat': 36,\n",
       " 'southwestern': 37,\n",
       " 'coexist': 38,\n",
       " 'Delegations': 39,\n",
       " 'Think': 40,\n",
       " 'brutalized': 41,\n",
       " 'industry-backed': 42,\n",
       " 'Ivanov': 43,\n",
       " 'Memoir': 44,\n",
       " 'robust': 45,\n",
       " 'Yong-Chun': 46,\n",
       " 'rocketed': 47,\n",
       " 'step': 48,\n",
       " 'fuss': 49,\n",
       " 'Mohammadi': 50,\n",
       " 'Meishan': 51,\n",
       " 'Lund': 52,\n",
       " 'kicks': 53,\n",
       " 'burning': 54,\n",
       " 'Ion': 55,\n",
       " 'effected': 56,\n",
       " 'inter-religious': 57,\n",
       " 'Keesler': 58,\n",
       " 'Qiang': 59,\n",
       " 'Christa': 60,\n",
       " 'Refugees': 61,\n",
       " 'assessment': 62,\n",
       " 'Kennedys': 63,\n",
       " 'Abdoulaye': 64,\n",
       " 'golden': 65,\n",
       " 'socio-economic': 66,\n",
       " 'blossomed': 67,\n",
       " 'qualify': 68,\n",
       " 'skier': 69,\n",
       " 'Padilla': 70,\n",
       " 'barracks': 71,\n",
       " 'test-fired': 72,\n",
       " 'high-altitude': 73,\n",
       " 'Cykla': 74,\n",
       " 'regulates': 75,\n",
       " 'Dandy': 76,\n",
       " 'Eastern': 77,\n",
       " 'nurse': 78,\n",
       " 'surgeries': 79,\n",
       " 'nine-year': 80,\n",
       " 'batsman': 81,\n",
       " 'tumor': 82,\n",
       " 'pretrial': 83,\n",
       " 'prestigious': 84,\n",
       " '31st': 85,\n",
       " 'Russian-Georgian': 86,\n",
       " 'restore': 87,\n",
       " '42-year-old': 88,\n",
       " 'Abdel-Al': 89,\n",
       " 'separated': 90,\n",
       " 'woman': 91,\n",
       " 'Babaker': 92,\n",
       " 'Dahir': 93,\n",
       " 'disillusionment': 94,\n",
       " 'Ki': 95,\n",
       " 'Ntini': 96,\n",
       " 'Faroese': 97,\n",
       " 'rigs': 98,\n",
       " 'Restaurant': 99,\n",
       " 'Separate': 100,\n",
       " 'technique': 101,\n",
       " 'Margraten': 102,\n",
       " 'Wanzhou': 103,\n",
       " '4th': 104,\n",
       " 'Cholera': 105,\n",
       " 'D': 106,\n",
       " 'overcharges': 107,\n",
       " 'discrepancies': 108,\n",
       " 'LaBeouf': 109,\n",
       " 'Bujumbura': 110,\n",
       " 'requests': 111,\n",
       " 'imposes': 112,\n",
       " 'granite': 113,\n",
       " 'Kleinkirchheim': 114,\n",
       " 'Lieutenant': 115,\n",
       " 'Preval': 116,\n",
       " 'representation': 117,\n",
       " 'upside': 118,\n",
       " 'Gana': 119,\n",
       " 'Shadow': 120,\n",
       " 'high-security': 121,\n",
       " 'DOMBROVSKIS': 122,\n",
       " 'NME': 123,\n",
       " 'indulging': 124,\n",
       " 'statutes': 125,\n",
       " 'greenhouse': 126,\n",
       " 'Daya': 127,\n",
       " 'AC-130': 128,\n",
       " 'Jolo': 129,\n",
       " 'heels': 130,\n",
       " 'homemade': 131,\n",
       " 'Babri': 132,\n",
       " 'relinquish': 133,\n",
       " 'hast': 134,\n",
       " 'tempered': 135,\n",
       " 'Ilam': 136,\n",
       " '76.7': 137,\n",
       " 'scathing': 138,\n",
       " 'Ohanesian': 139,\n",
       " 'row': 140,\n",
       " 'lives': 141,\n",
       " 'squadron': 142,\n",
       " 'denuclearize': 143,\n",
       " 'massacre': 144,\n",
       " 'disposition': 145,\n",
       " '1796': 146,\n",
       " 'classes': 147,\n",
       " 'phase': 148,\n",
       " 'erupted': 149,\n",
       " 'Loveland': 150,\n",
       " 'Mutahida': 151,\n",
       " 'European-brokered': 152,\n",
       " 'Houses': 153,\n",
       " 'shrinks': 154,\n",
       " 'Ignace': 155,\n",
       " 'springboard': 156,\n",
       " 'five-story': 157,\n",
       " 'rock': 158,\n",
       " 'anti-genetic': 159,\n",
       " 'Janet': 160,\n",
       " 'bachelor': 161,\n",
       " 'Astros': 162,\n",
       " '982': 163,\n",
       " 'walking': 164,\n",
       " 'Hussein': 165,\n",
       " 'skulked': 166,\n",
       " 'injection': 167,\n",
       " 'confrontation': 168,\n",
       " 'Heathrow': 169,\n",
       " 'obligations': 170,\n",
       " 'Lendu': 171,\n",
       " 'Guard': 172,\n",
       " 'heinous': 173,\n",
       " 'southernmost': 174,\n",
       " 'Ludlam': 175,\n",
       " '1,500-meter': 176,\n",
       " 'overpopulated': 177,\n",
       " 'ago': 178,\n",
       " 'PHILOSOPHER': 179,\n",
       " 'commanders': 180,\n",
       " 'sensitivities': 181,\n",
       " 'Sierra': 182,\n",
       " '373': 183,\n",
       " 'Kareem': 184,\n",
       " 'pegged': 185,\n",
       " 'Adre': 186,\n",
       " 'Street': 187,\n",
       " 'Center-left': 188,\n",
       " 'echoing': 189,\n",
       " 'Maruf': 190,\n",
       " 'UNHCR': 191,\n",
       " 'Schneiderhan': 192,\n",
       " 'two-dozen': 193,\n",
       " 'pasted': 194,\n",
       " 'patrols': 195,\n",
       " 'Sixto': 196,\n",
       " 'Belt': 197,\n",
       " 'browsing': 198,\n",
       " 'contrasts': 199,\n",
       " 'main': 200,\n",
       " 'depictions': 201,\n",
       " 'Prommegger': 202,\n",
       " 'permanent': 203,\n",
       " 'Neuilly': 204,\n",
       " 'ordeal': 205,\n",
       " 'Evacuees': 206,\n",
       " 'Blechschmidt': 207,\n",
       " 'Concertacion': 208,\n",
       " 'sheltered': 209,\n",
       " 'Union-U.N.': 210,\n",
       " 'wetlands': 211,\n",
       " 'Hydropower': 212,\n",
       " '1920s': 213,\n",
       " 'Roddick': 214,\n",
       " 'Odense': 215,\n",
       " 'vowed': 216,\n",
       " 'Josefa': 217,\n",
       " 'Ohno': 218,\n",
       " 'Nikola': 219,\n",
       " 'dispatching': 220,\n",
       " 'exacerbates': 221,\n",
       " 'Arabia': 222,\n",
       " 'G20': 223,\n",
       " 'princes': 224,\n",
       " 'Moscow-leaning': 225,\n",
       " 'lambs': 226,\n",
       " 'Taif': 227,\n",
       " 'parents': 228,\n",
       " 'Aberrahman': 229,\n",
       " 'Cheddi': 230,\n",
       " 'imaginary': 231,\n",
       " 'mercantile': 232,\n",
       " 'Naeemi': 233,\n",
       " 'deadly': 234,\n",
       " 'unofficial': 235,\n",
       " 'industry': 236,\n",
       " 'Prodi': 237,\n",
       " 'help': 238,\n",
       " 'Micky': 239,\n",
       " 'two-month': 240,\n",
       " 'supplemental': 241,\n",
       " 'Lion': 242,\n",
       " 'relocation': 243,\n",
       " 'Melville': 244,\n",
       " 'discourages': 245,\n",
       " 'wallowing': 246,\n",
       " 'Exports': 247,\n",
       " 'Azocar': 248,\n",
       " 'Let': 249,\n",
       " 'Sinmun': 250,\n",
       " 'Omaha': 251,\n",
       " 'easily': 252,\n",
       " '404': 253,\n",
       " 'Kozulin': 254,\n",
       " 'Outgoing': 255,\n",
       " 'wedded': 256,\n",
       " 'reading': 257,\n",
       " 'Longer-term': 258,\n",
       " 'non-Islamic': 259,\n",
       " 'reorganize': 260,\n",
       " 'Subsequently': 261,\n",
       " 'histories': 262,\n",
       " 'besought': 263,\n",
       " 'Zanzibar': 264,\n",
       " 'Issawiya': 265,\n",
       " 'thrived': 266,\n",
       " 'finger': 267,\n",
       " 'melons': 268,\n",
       " 'formed': 269,\n",
       " 'peacekeeers': 270,\n",
       " 'Gustav': 271,\n",
       " 'charities': 272,\n",
       " 'intentionally': 273,\n",
       " '39-29': 274,\n",
       " 'PETER': 275,\n",
       " 'Rompuy': 276,\n",
       " 'medium': 277,\n",
       " 'fields': 278,\n",
       " 'al-Uloum': 279,\n",
       " 'unreliable': 280,\n",
       " '1755': 281,\n",
       " 'commenced': 282,\n",
       " 'plaque': 283,\n",
       " 'Samantha': 284,\n",
       " 'Goldsmith': 285,\n",
       " 'whatever': 286,\n",
       " 'Hassem': 287,\n",
       " 'legendary': 288,\n",
       " 'halts': 289,\n",
       " 'Control': 290,\n",
       " '1633': 291,\n",
       " 'Son': 292,\n",
       " 'article': 293,\n",
       " 'hard': 294,\n",
       " '72.64': 295,\n",
       " 'retailers': 296,\n",
       " 'oats': 297,\n",
       " 'Radovan': 298,\n",
       " '4.11': 299,\n",
       " 'Saffir-Simpson': 300,\n",
       " 'U.N.-Congolese': 301,\n",
       " 'flexibility': 302,\n",
       " 'spread': 303,\n",
       " '297': 304,\n",
       " 'Hadj': 305,\n",
       " 'Huckabee': 306,\n",
       " 'Samir': 307,\n",
       " 'Halutz': 308,\n",
       " 'Hourmadji': 309,\n",
       " 'Bussereau': 310,\n",
       " 'CCTV': 311,\n",
       " 'abandon': 312,\n",
       " 'principally': 313,\n",
       " 'Downing': 314,\n",
       " 'Xingning': 315,\n",
       " 'Giuliani': 316,\n",
       " 'distanced': 317,\n",
       " 'migrated': 318,\n",
       " 'unravel': 319,\n",
       " 'Ramush': 320,\n",
       " 'five-star': 321,\n",
       " 'Samahdna': 322,\n",
       " 'Matthias': 323,\n",
       " 'Waldron': 324,\n",
       " 'study': 325,\n",
       " 'Hannah': 326,\n",
       " 'commemorate': 327,\n",
       " 'minus': 328,\n",
       " 'harden': 329,\n",
       " 'Murillo': 330,\n",
       " 'cult': 331,\n",
       " 'gracious': 332,\n",
       " 'spite': 333,\n",
       " 'wild': 334,\n",
       " \"'\": 335,\n",
       " 'differences': 336,\n",
       " 'archer': 337,\n",
       " '67,000': 338,\n",
       " 'dissatisified': 339,\n",
       " 'disruptions': 340,\n",
       " 'halfway': 341,\n",
       " 'homeland': 342,\n",
       " 'ice-covered': 343,\n",
       " 'failing': 344,\n",
       " 'hospital': 345,\n",
       " \"Ma'ariv\": 346,\n",
       " 'Godfather': 347,\n",
       " 'Shepherd': 348,\n",
       " 'remixed': 349,\n",
       " 'tariff-free': 350,\n",
       " '106': 351,\n",
       " '24': 352,\n",
       " 'garnering': 353,\n",
       " 'farms': 354,\n",
       " 'Teheran': 355,\n",
       " 'righteous': 356,\n",
       " 'Such': 357,\n",
       " 'Najibullah': 358,\n",
       " 'insufficient': 359,\n",
       " 'highlight': 360,\n",
       " 'underinvestment': 361,\n",
       " 'Chechnya': 362,\n",
       " 'Shiv': 363,\n",
       " 'hanging': 364,\n",
       " 'Islamist-controlled': 365,\n",
       " 'FLEA': 366,\n",
       " 'HAMSAT': 367,\n",
       " 'Mironov': 368,\n",
       " 'receipts': 369,\n",
       " 'repeatedly': 370,\n",
       " 'Galan': 371,\n",
       " 'attorney-general': 372,\n",
       " 'authoritarian': 373,\n",
       " 'Adhamiyah': 374,\n",
       " 'uprisings': 375,\n",
       " 'Arnold': 376,\n",
       " '1.329': 377,\n",
       " 'city': 378,\n",
       " 'oversupply': 379,\n",
       " 'Color': 380,\n",
       " 'comparison': 381,\n",
       " 'migrants': 382,\n",
       " 'applies': 383,\n",
       " 'hearts': 384,\n",
       " 'unacceptably': 385,\n",
       " 'kilowatt': 386,\n",
       " 'lawlessness': 387,\n",
       " 'Peng': 388,\n",
       " 'feed': 389,\n",
       " 'Diyala': 390,\n",
       " 'Zhaoxing': 391,\n",
       " 'Volver': 392,\n",
       " 'absorbs': 393,\n",
       " 'drives': 394,\n",
       " 'Gong': 395,\n",
       " 'solely': 396,\n",
       " 'signboard': 397,\n",
       " 'restrictions': 398,\n",
       " 'Mac': 399,\n",
       " 'ILNA': 400,\n",
       " 'Rajasthan': 401,\n",
       " 'Dinari': 402,\n",
       " '1895': 403,\n",
       " 'weird': 404,\n",
       " 'market-oriented': 405,\n",
       " 'Monrovia': 406,\n",
       " 'starts': 407,\n",
       " 'nearer': 408,\n",
       " '44': 409,\n",
       " 'moderates': 410,\n",
       " 'Gillespie': 411,\n",
       " 'Shareholders': 412,\n",
       " 'oversized': 413,\n",
       " 'flashpoint': 414,\n",
       " 'renowned': 415,\n",
       " 'Alexeyeva': 416,\n",
       " 'Dramane': 417,\n",
       " 'rivaling': 418,\n",
       " 'Dealing': 419,\n",
       " 'communist-led': 420,\n",
       " 'resting': 421,\n",
       " 'VENETIAAN': 422,\n",
       " 'capital-intensive': 423,\n",
       " 'humane': 424,\n",
       " 'bargain': 425,\n",
       " 'Stanczak': 426,\n",
       " 'Yoweri': 427,\n",
       " 'Janica': 428,\n",
       " 'Canadian-born': 429,\n",
       " 'upwards': 430,\n",
       " 'Gerolsteiner': 431,\n",
       " '60-thousand': 432,\n",
       " 'NDC': 433,\n",
       " 'farmland': 434,\n",
       " 'watches': 435,\n",
       " 'junk': 436,\n",
       " 'Nesnera': 437,\n",
       " 'U.S.-Mexico': 438,\n",
       " 'Beach': 439,\n",
       " 'Woodman': 440,\n",
       " 'supplement': 441,\n",
       " 'Outdoors': 442,\n",
       " 'mistrial': 443,\n",
       " 'lamb': 444,\n",
       " 'Ludin': 445,\n",
       " 'sheltering': 446,\n",
       " 'APS': 447,\n",
       " 'Denmark-Sweden': 448,\n",
       " 'everyday': 449,\n",
       " 'far-reaching': 450,\n",
       " 'Ceremony': 451,\n",
       " 'interferes': 452,\n",
       " 'Lukashenko': 453,\n",
       " 'disrespectfully': 454,\n",
       " '2006': 455,\n",
       " 'slid': 456,\n",
       " 'Insurance': 457,\n",
       " 'two-week-old': 458,\n",
       " 'Correa': 459,\n",
       " 'Airlift': 460,\n",
       " 'Sumaidaie': 461,\n",
       " 'Data': 462,\n",
       " 'Authorized': 463,\n",
       " 'reins': 464,\n",
       " 'Purple': 465,\n",
       " 'employers': 466,\n",
       " 'unfolded': 467,\n",
       " 'Bwakira': 468,\n",
       " 'Hang': 469,\n",
       " 'enough': 470,\n",
       " 'prosecutors': 471,\n",
       " 'flames': 472,\n",
       " 'Cyril': 473,\n",
       " 'Ice': 474,\n",
       " 'workhorse': 475,\n",
       " 'Phuket': 476,\n",
       " 'disciplinary': 477,\n",
       " 'bib': 478,\n",
       " 'blockade': 479,\n",
       " 'PFLP': 480,\n",
       " 'Irish': 481,\n",
       " 'Bextra': 482,\n",
       " 'tents': 483,\n",
       " 'Mikasa': 484,\n",
       " 'typically': 485,\n",
       " 'hired': 486,\n",
       " 'free-trade': 487,\n",
       " 'F-5': 488,\n",
       " 'Global': 489,\n",
       " 'midfield': 490,\n",
       " 'darkest': 491,\n",
       " 'CASTRO': 492,\n",
       " 'earthquakes': 493,\n",
       " 'restrictive': 494,\n",
       " 'minds': 495,\n",
       " 'spied': 496,\n",
       " 'Aleppo': 497,\n",
       " 'network': 498,\n",
       " 'ejected': 499,\n",
       " 'destiny': 500,\n",
       " 'pain': 501,\n",
       " 'Gen.': 502,\n",
       " 're-scheduled': 503,\n",
       " 'marginalized': 504,\n",
       " 'Archer': 505,\n",
       " 'pancreatic': 506,\n",
       " 'Bourgas': 507,\n",
       " 'concrete': 508,\n",
       " 'black-clad': 509,\n",
       " 'Financial': 510,\n",
       " 'Inc.': 511,\n",
       " 'Roses': 512,\n",
       " 'underemployment': 513,\n",
       " 'McCormack': 514,\n",
       " 'preferences': 515,\n",
       " 'hosts': 516,\n",
       " 'villager': 517,\n",
       " 'mid-week': 518,\n",
       " 'Brownback': 519,\n",
       " 'traitors': 520,\n",
       " 'trespassing': 521,\n",
       " '1,300': 522,\n",
       " 'murdering': 523,\n",
       " 'singing': 524,\n",
       " 'Kruif': 525,\n",
       " '216-3': 526,\n",
       " 'deal': 527,\n",
       " 'aerial': 528,\n",
       " 'six-and-a-half': 529,\n",
       " 'sharpened': 530,\n",
       " 'dearest': 531,\n",
       " 'Desai': 532,\n",
       " 'sold': 533,\n",
       " 'Forges': 534,\n",
       " 'jumped': 535,\n",
       " '9th': 536,\n",
       " 'Notice': 537,\n",
       " 'al-Baghdadi': 538,\n",
       " 'midterm': 539,\n",
       " 'wept': 540,\n",
       " 'Majmaah': 541,\n",
       " 'Property': 542,\n",
       " 'Dirceu': 543,\n",
       " 'Tegucigalpa': 544,\n",
       " 'salary': 545,\n",
       " 'cathedral': 546,\n",
       " 'Segolene': 547,\n",
       " 'Bugti': 548,\n",
       " 'cousins': 549,\n",
       " 'celebrates': 550,\n",
       " 'overrule': 551,\n",
       " 'being': 552,\n",
       " 'Talat': 553,\n",
       " 'canine': 554,\n",
       " 'Adelie': 555,\n",
       " 'carryover': 556,\n",
       " '76.4': 557,\n",
       " 'al-Tayyah': 558,\n",
       " 'Aso': 559,\n",
       " 'grudgingly': 560,\n",
       " '78.4': 561,\n",
       " 'Russian-built': 562,\n",
       " 'Tahar': 563,\n",
       " 'Deere': 564,\n",
       " 'al-Mustaqbal': 565,\n",
       " '55-member': 566,\n",
       " 'razor': 567,\n",
       " 'Virunga': 568,\n",
       " 'Rizvan': 569,\n",
       " 'westerners': 570,\n",
       " 'devout': 571,\n",
       " 'commuter': 572,\n",
       " 'Getty': 573,\n",
       " 'Kaplan': 574,\n",
       " 'change': 575,\n",
       " 'Bakri': 576,\n",
       " 'apostates': 577,\n",
       " 'mother-to-child': 578,\n",
       " 'daytime': 579,\n",
       " 'Influential': 580,\n",
       " 'well-placed': 581,\n",
       " 'SHIPWRECKED': 582,\n",
       " 'anticipates': 583,\n",
       " 'convenes': 584,\n",
       " 'course': 585,\n",
       " 'poorer': 586,\n",
       " 'Todd': 587,\n",
       " 'Ataturk': 588,\n",
       " 'equaling': 589,\n",
       " 'home': 590,\n",
       " 'jails': 591,\n",
       " 'H.W.': 592,\n",
       " 'stumbling': 593,\n",
       " 'Portland': 594,\n",
       " 'regarding': 595,\n",
       " 'Qom': 596,\n",
       " 'Paris': 597,\n",
       " 'Caroni': 598,\n",
       " 'commodities': 599,\n",
       " 'believed': 600,\n",
       " 'comparable': 601,\n",
       " 'seven-point': 602,\n",
       " 'ban': 603,\n",
       " 'jolted': 604,\n",
       " 'Indigenous': 605,\n",
       " 'dear': 606,\n",
       " 'Altria': 607,\n",
       " 'best-preserved': 608,\n",
       " 'Schoomaker': 609,\n",
       " 'muted': 610,\n",
       " 'Hanoi': 611,\n",
       " 'catalog': 612,\n",
       " 'Wars': 613,\n",
       " '178': 614,\n",
       " 'precaution': 615,\n",
       " 'downs': 616,\n",
       " 'fibroids': 617,\n",
       " '122': 618,\n",
       " 'patrol': 619,\n",
       " 'Egypt-Gaza': 620,\n",
       " 'Randolph': 621,\n",
       " 'Schumer': 622,\n",
       " 'Rignot': 623,\n",
       " 'stroked': 624,\n",
       " 'confronted': 625,\n",
       " 'SA': 626,\n",
       " 'chart': 627,\n",
       " 'PEJAK': 628,\n",
       " '01-Feb': 629,\n",
       " 'Ackerman': 630,\n",
       " 'Fuji': 631,\n",
       " 'Marcinkiewicz': 632,\n",
       " 'sensitive': 633,\n",
       " '10-month-old': 634,\n",
       " 'FRG': 635,\n",
       " 'Betar': 636,\n",
       " '30,00,000': 637,\n",
       " 'Mughal': 638,\n",
       " 'life-threatening': 639,\n",
       " 'studio': 640,\n",
       " 'warrant': 641,\n",
       " 'candle': 642,\n",
       " 'Kostiw': 643,\n",
       " 'indications': 644,\n",
       " 'oil-and-gas': 645,\n",
       " 'Neiva': 646,\n",
       " 'Hubble': 647,\n",
       " '1,460': 648,\n",
       " 'ZANU-PF': 649,\n",
       " 'urges': 650,\n",
       " 'Belet': 651,\n",
       " 'airlifting': 652,\n",
       " 'reservists': 653,\n",
       " 'Zabi': 654,\n",
       " 'Kremlin-controlled': 655,\n",
       " 'witness': 656,\n",
       " 'turbulent': 657,\n",
       " '18.7': 658,\n",
       " 'nevirapine': 659,\n",
       " 'short-wave': 660,\n",
       " 'Pradip': 661,\n",
       " 'urinary': 662,\n",
       " 'Mahmud': 663,\n",
       " 'elaborating': 664,\n",
       " 'Dunham': 665,\n",
       " 'enjoys': 666,\n",
       " 'Dairy': 667,\n",
       " 'massive': 668,\n",
       " 'condone': 669,\n",
       " 'plea': 670,\n",
       " 'reopens': 671,\n",
       " 'rode': 672,\n",
       " 'Isle': 673,\n",
       " 'expelling': 674,\n",
       " 'fashioning': 675,\n",
       " 'enroute': 676,\n",
       " 'hailed': 677,\n",
       " 'Seahawks': 678,\n",
       " '1952': 679,\n",
       " 'Hoekstra': 680,\n",
       " 'possibilities': 681,\n",
       " '23-year-old': 682,\n",
       " 'Dudley': 683,\n",
       " '6,23,000': 684,\n",
       " 'unclaimed': 685,\n",
       " 'Barbuda': 686,\n",
       " 'Finnish': 687,\n",
       " 'valuable': 688,\n",
       " '174': 689,\n",
       " 'event': 690,\n",
       " 'resettlement': 691,\n",
       " 'substitute': 692,\n",
       " 'principalities': 693,\n",
       " 'Ending': 694,\n",
       " 'Sezer': 695,\n",
       " 'editorial': 696,\n",
       " 'over-the': 697,\n",
       " 'corridor': 698,\n",
       " 'iron': 699,\n",
       " \"chang'aa\": 700,\n",
       " 'Bailey': 701,\n",
       " 'intent': 702,\n",
       " 'Conakry': 703,\n",
       " 'NYYAZOW': 704,\n",
       " 'spending': 705,\n",
       " 'agitated': 706,\n",
       " 'stretch': 707,\n",
       " 'debenture': 708,\n",
       " 'Fan': 709,\n",
       " 'citizen': 710,\n",
       " '50.56': 711,\n",
       " 'Kartli-Iberia': 712,\n",
       " 'Mahinda': 713,\n",
       " 'Masters': 714,\n",
       " 'Insured': 715,\n",
       " 'gusts': 716,\n",
       " 'finals': 717,\n",
       " 'sub-contractor': 718,\n",
       " 'Dominik': 719,\n",
       " 'Plavia': 720,\n",
       " 'Jeffery': 721,\n",
       " 'prepares': 722,\n",
       " 'Brahimaj': 723,\n",
       " 're-issued': 724,\n",
       " 'Gunsmoke': 725,\n",
       " 'Faruq': 726,\n",
       " 'involve': 727,\n",
       " 'birth': 728,\n",
       " 'punished': 729,\n",
       " 'catalyst': 730,\n",
       " 'Serena': 731,\n",
       " 'Vahid': 732,\n",
       " 'played': 733,\n",
       " 'top-level': 734,\n",
       " 'Triumf': 735,\n",
       " 'phoenixes': 736,\n",
       " 'Boeing': 737,\n",
       " 'slit': 738,\n",
       " 'Sebastiao': 739,\n",
       " 'divorce': 740,\n",
       " 'Jamaat-i-Islami': 741,\n",
       " 'alternates': 742,\n",
       " 'Jammu-Kashmir': 743,\n",
       " 'Katharine': 744,\n",
       " 'Coomaraswamy': 745,\n",
       " 'ineffectively': 746,\n",
       " 'Sahel': 747,\n",
       " 'transfusion': 748,\n",
       " 'Medvedev': 749,\n",
       " 'Salas': 750,\n",
       " 'solutions': 751,\n",
       " '737-800': 752,\n",
       " '42,000': 753,\n",
       " 'definitely': 754,\n",
       " 'life-and-death': 755,\n",
       " 'U-2': 756,\n",
       " 'lips': 757,\n",
       " 'Sarta': 758,\n",
       " 'Hamburg': 759,\n",
       " 'Band': 760,\n",
       " 'Al-Islam': 761,\n",
       " 'LEE': 762,\n",
       " 'periods': 763,\n",
       " 'marshy': 764,\n",
       " 'Tiantian': 765,\n",
       " 'applause': 766,\n",
       " 'richly': 767,\n",
       " 'P-50': 768,\n",
       " 'swearing': 769,\n",
       " 'Amed': 770,\n",
       " '8,50,000': 771,\n",
       " 'Youssef': 772,\n",
       " '500-kilogram': 773,\n",
       " 'concentrated': 774,\n",
       " 'raid': 775,\n",
       " 'speak': 776,\n",
       " 'demeans': 777,\n",
       " '5.1-magnitude': 778,\n",
       " 'candidates': 779,\n",
       " 'deputy': 780,\n",
       " 'waterfowl': 781,\n",
       " 'E.U.': 782,\n",
       " 'Murungi': 783,\n",
       " 'physiology': 784,\n",
       " 'al-Tahreer': 785,\n",
       " 'Marion': 786,\n",
       " 'generates': 787,\n",
       " 'firsts': 788,\n",
       " 'Proposals': 789,\n",
       " 'Kafr': 790,\n",
       " 'Bajur': 791,\n",
       " 'four-sets': 792,\n",
       " 'Ambitious': 793,\n",
       " 'scenes': 794,\n",
       " 'bets': 795,\n",
       " 'Schwehm': 796,\n",
       " 'Detection': 797,\n",
       " 'fluttered': 798,\n",
       " 'suicide-bomber': 799,\n",
       " 'co-principality': 800,\n",
       " 'puts': 801,\n",
       " 'mid-October': 802,\n",
       " 'Eyes': 803,\n",
       " 'awkward': 804,\n",
       " 'Qasim': 805,\n",
       " '17-Jun': 806,\n",
       " 'Dastjerdi': 807,\n",
       " 'Mother': 808,\n",
       " 'Tens': 809,\n",
       " 'reunions': 810,\n",
       " 'apologize': 811,\n",
       " 'attracting': 812,\n",
       " 'learning': 813,\n",
       " 'sweltering': 814,\n",
       " 'Mitofsky': 815,\n",
       " 'utilization': 816,\n",
       " 'Itno': 817,\n",
       " 'respirator': 818,\n",
       " 'bazaar': 819,\n",
       " 'ex-Yukos': 820,\n",
       " 'Italian': 821,\n",
       " 'kites': 822,\n",
       " 'mechanics': 823,\n",
       " 'retailer': 824,\n",
       " 'Arturo': 825,\n",
       " 'Legislation': 826,\n",
       " 'captives': 827,\n",
       " 'Khalil': 828,\n",
       " 'Jalalzadeh': 829,\n",
       " 'Tunis': 830,\n",
       " 'Verkhovna': 831,\n",
       " 'Musab': 832,\n",
       " 'Iscuande': 833,\n",
       " 'timelines': 834,\n",
       " 'Horseman': 835,\n",
       " 'dazzling': 836,\n",
       " 'tax': 837,\n",
       " 'Fenty': 838,\n",
       " '(': 839,\n",
       " 'Chitral': 840,\n",
       " 'Glacier': 841,\n",
       " 'treated': 842,\n",
       " 'timing': 843,\n",
       " 'tracked': 844,\n",
       " 'vomit': 845,\n",
       " 'Embera': 846,\n",
       " 'Unmasking': 847,\n",
       " 'disruption': 848,\n",
       " 'Terje': 849,\n",
       " 'surprise': 850,\n",
       " 'Iran-Oman': 851,\n",
       " 'dance': 852,\n",
       " 'strongest-ever': 853,\n",
       " 'roofs': 854,\n",
       " 'abolition': 855,\n",
       " 'pair': 856,\n",
       " 'tone': 857,\n",
       " 'temperature': 858,\n",
       " 'disproportionally': 859,\n",
       " 'Copper': 860,\n",
       " 'Envoys': 861,\n",
       " 'ourselves': 862,\n",
       " 'Mukherjee': 863,\n",
       " 'Kurds': 864,\n",
       " 'encountered': 865,\n",
       " 'Petra': 866,\n",
       " 'lifeline': 867,\n",
       " 'high-income': 868,\n",
       " 'Challengers': 869,\n",
       " 'annexation': 870,\n",
       " 'Sangju': 871,\n",
       " '11-year-old': 872,\n",
       " 'Kibembi': 873,\n",
       " '2000': 874,\n",
       " 'Jowell': 875,\n",
       " 'bone': 876,\n",
       " 'extreme': 877,\n",
       " 'Radek': 878,\n",
       " 'duties': 879,\n",
       " 'distorts': 880,\n",
       " 'leftists': 881,\n",
       " 'depressing': 882,\n",
       " '87.61': 883,\n",
       " 'still': 884,\n",
       " '52-year-old': 885,\n",
       " 'Investigators': 886,\n",
       " '90th': 887,\n",
       " '3-year': 888,\n",
       " 'Administrator': 889,\n",
       " 'Counterterrorism': 890,\n",
       " 'poorest': 891,\n",
       " 'terrorism-related': 892,\n",
       " 'expires': 893,\n",
       " '12.5': 894,\n",
       " 'distracted': 895,\n",
       " '3-point': 896,\n",
       " 'Fighters': 897,\n",
       " 'Non-Violent': 898,\n",
       " 'overtures': 899,\n",
       " 'Mikhailova': 900,\n",
       " 'farther': 901,\n",
       " 'dunking': 902,\n",
       " 'airstrip': 903,\n",
       " 'sold-out': 904,\n",
       " 'Zelenovic': 905,\n",
       " 'impede': 906,\n",
       " 'plenary': 907,\n",
       " 'Suspects': 908,\n",
       " 'rapid-reaction': 909,\n",
       " 'Ortiz': 910,\n",
       " 'Henin': 911,\n",
       " 'shipwreck': 912,\n",
       " 'Baghran': 913,\n",
       " 'which': 914,\n",
       " 'housing': 915,\n",
       " 'struggled': 916,\n",
       " 'al-Maliki': 917,\n",
       " 'lashed': 918,\n",
       " 'Reykjavik': 919,\n",
       " 'equip': 920,\n",
       " 'lieutenant': 921,\n",
       " 'super-combi': 922,\n",
       " 'trades': 923,\n",
       " 'Faculty': 924,\n",
       " 'shall': 925,\n",
       " 'kiosks': 926,\n",
       " 'Saeed': 927,\n",
       " 'intra-Palestinian': 928,\n",
       " 'But': 929,\n",
       " 'performers': 930,\n",
       " 'censors': 931,\n",
       " 'capture': 932,\n",
       " 'roaring': 933,\n",
       " 'dereliction': 934,\n",
       " 'Ancient': 935,\n",
       " 'shutting': 936,\n",
       " 'barges': 937,\n",
       " 'water-dropping': 938,\n",
       " 'Gregg': 939,\n",
       " 'English-language': 940,\n",
       " 'potentially': 941,\n",
       " 'blaming': 942,\n",
       " 'seizures': 943,\n",
       " 'bypassed': 944,\n",
       " 'anti-doping': 945,\n",
       " 'lavish': 946,\n",
       " 'strongest': 947,\n",
       " 'catch': 948,\n",
       " 'heater': 949,\n",
       " 'knowledge': 950,\n",
       " 'pro-reunification': 951,\n",
       " 'hammering': 952,\n",
       " 'medium-term': 953,\n",
       " 'Negotiations': 954,\n",
       " 'QABOOS': 955,\n",
       " 'reporting': 956,\n",
       " 'redirect': 957,\n",
       " 'cost-reduction': 958,\n",
       " 'Singnaghi': 959,\n",
       " 'SLA': 960,\n",
       " 'Supreme': 961,\n",
       " 'snow-covered': 962,\n",
       " 'newly-built': 963,\n",
       " 'Banks': 964,\n",
       " 'Esperon': 965,\n",
       " 'government-appointed': 966,\n",
       " 'informal': 967,\n",
       " 'bar': 968,\n",
       " 'Cable': 969,\n",
       " 'lower-level': 970,\n",
       " 'Beerenberg': 971,\n",
       " 'landmarks': 972,\n",
       " 'asleep': 973,\n",
       " 'Iginla': 974,\n",
       " 'clamps': 975,\n",
       " 'prosecutor': 976,\n",
       " 'reconstituted': 977,\n",
       " '40': 978,\n",
       " 'Zagreb': 979,\n",
       " 'Manmohan': 980,\n",
       " 'Irbil': 981,\n",
       " 'Lustiger': 982,\n",
       " 'Housing': 983,\n",
       " 'relaxed': 984,\n",
       " '1494': 985,\n",
       " '8,000-strong': 986,\n",
       " 'Kousalyan': 987,\n",
       " 'turb': 988,\n",
       " 'Judaidah': 989,\n",
       " 'priestly': 990,\n",
       " 'researcher': 991,\n",
       " 'indissoluble': 992,\n",
       " 'RosUkrEnergo': 993,\n",
       " 'quelling': 994,\n",
       " 'Alec': 995,\n",
       " 'wit': 996,\n",
       " 'funneling': 997,\n",
       " 'ensued': 998,\n",
       " 'count': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8728fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Thousands\n",
       "1                of\n",
       "2     demonstrators\n",
       "3              have\n",
       "4           marched\n",
       "5           through\n",
       "6            London\n",
       "7                to\n",
       "8           protest\n",
       "9               the\n",
       "10              war\n",
       "11               in\n",
       "12             Iraq\n",
       "13              and\n",
       "14           demand\n",
       "15              the\n",
       "16       withdrawal\n",
       "17               of\n",
       "18          British\n",
       "19           troops\n",
       "20             from\n",
       "21             that\n",
       "22          country\n",
       "23                .\n",
       "24         Families\n",
       "Name: Word, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Word[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bfbeb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          29922\n",
       "1           7340\n",
       "2           2338\n",
       "3           8951\n",
       "4           5941\n",
       "           ...  \n",
       "1048570     7156\n",
       "1048571    28089\n",
       "1048572    24026\n",
       "1048573    31351\n",
       "1048574     7546\n",
       "Name: Word, Length: 1048575, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aee6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [x for x in data.Word if x == 'of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b95a3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26354"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bd28304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61244/1322379973.py:5: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  data_group = data_fillna.groupby(\n"
     ]
    }
   ],
   "source": [
    "data['Word_idx'] = data['Word'].map(token2idx)\n",
    "data['Tag_idx'] = data['Tag'].map(tag2idx)\n",
    "data_fillna = data.fillna(method='ffill', axis=0)\n",
    "# Groupby and collect columns\n",
    "data_group = data_fillna.groupby(\n",
    "['Sentence #'],as_index=False\n",
    ")['Word', 'POS', 'Tag', 'Word_idx', 'Tag_idx'].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc688649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 14:49:03.609110: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-01 14:49:03.609155: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 32372 \n",
      "train_tokens length: 32372 \n",
      "test_tokens length: 4796 \n",
      "test_tags: 4796 \n",
      "val_tokens: 10791 \n",
      "val_tags: 10791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def get_pad_train_test_val(data_group, data):\n",
    "\n",
    "    #get max token and tag length\n",
    "    n_token = len(list(set(data['Word'].to_list())))\n",
    "    n_tag = len(list(set(data['Tag'].to_list())))\n",
    "\n",
    "    #Pad tokens (X var)    \n",
    "    tokens = data_group['Word_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "\n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['Tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    #Split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "\n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntrain_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "        '\\nval_tokens:', len(val_tokens),\n",
    "        '\\nval_tags:', len(val_tags),\n",
    "    )\n",
    "    \n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
    "\n",
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f83c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a0e1e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08f525ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(list(set(data['Word'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['Word_idx'].tolist()])\n",
    "n_tags = len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0af6a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "\n",
    "    #Optimiser \n",
    "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30c169c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    loss = list()\n",
    "    for i in range(25):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9f004e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 14:51:46.013232: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-09-01 14:51:46.013271: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-01 14:51:46.013305: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c4leb-PC): /proc/driver/nvidia/version does not exist\n",
      "2022-09-01 14:51:46.014291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 104, 64)           2251456   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 104, 128)         66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 104, 64)           49408     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 104, 17)          1105      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,368,017\n",
      "Trainable params: 2,368,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 14:51:46.879212: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 183143584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 98s 3s/step - loss: 1.1289 - accuracy: 0.9191 - val_loss: 0.3497 - val_accuracy: 0.9681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 14:54:16.828610: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 183143584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 91s 4s/step - loss: 0.2868 - accuracy: 0.9677 - val_loss: 0.2222 - val_accuracy: 0.9681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 14:55:48.086782: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 183143584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 88s 3s/step - loss: 0.2184 - accuracy: 0.9677 - val_loss: 0.1938 - val_accuracy: 0.9681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 14:57:16.546116: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 183143584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 138s 5s/step - loss: 0.1958 - accuracy: 0.9677 - val_loss: 0.1736 - val_accuracy: 0.9681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 14:59:38.642763: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 183143584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 95s 4s/step - loss: 0.2893 - accuracy: 0.9677 - val_loss: 0.2765 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 92s 4s/step - loss: 0.2093 - accuracy: 0.9677 - val_loss: 0.1805 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 90s 3s/step - loss: 0.1766 - accuracy: 0.9677 - val_loss: 0.1618 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 90s 3s/step - loss: 0.2445 - accuracy: 0.9677 - val_loss: 0.2310 - val_accuracy: 0.9682\n",
      "26/26 [==============================] - 87s 3s/step - loss: 0.2153 - accuracy: 0.9677 - val_loss: 0.1994 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 135s 5s/step - loss: 0.1852 - accuracy: 0.9677 - val_loss: 0.1650 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 104s 4s/step - loss: 0.1639 - accuracy: 0.9677 - val_loss: 0.1552 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 119s 5s/step - loss: 0.1587 - accuracy: 0.9677 - val_loss: 0.1573 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 165s 6s/step - loss: 0.4348 - accuracy: 0.9674 - val_loss: 0.2559 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 175s 7s/step - loss: 0.2257 - accuracy: 0.9677 - val_loss: 0.2004 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 145s 6s/step - loss: 0.1910 - accuracy: 0.9677 - val_loss: 0.1799 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 111s 4s/step - loss: 0.1735 - accuracy: 0.9677 - val_loss: 0.1611 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 118s 5s/step - loss: 0.1564 - accuracy: 0.9677 - val_loss: 0.1464 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 95s 4s/step - loss: 0.1493 - accuracy: 0.9677 - val_loss: 0.1415 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 103s 4s/step - loss: 0.1997 - accuracy: 0.9678 - val_loss: 0.2179 - val_accuracy: 0.9681\n",
      "26/26 [==============================] - 108s 4s/step - loss: 0.1920 - accuracy: 0.9678 - val_loss: 0.1736 - val_accuracy: 0.9682\n",
      "26/26 [==============================] - 94s 4s/step - loss: 0.1611 - accuracy: 0.9678 - val_loss: 0.1527 - val_accuracy: 0.9682\n",
      "26/26 [==============================] - 89s 3s/step - loss: 0.1437 - accuracy: 0.9678 - val_loss: 0.1412 - val_accuracy: 0.9682\n",
      "26/26 [==============================] - 89s 3s/step - loss: 0.1350 - accuracy: 0.9678 - val_loss: 0.1349 - val_accuracy: 0.9682\n",
      "26/26 [==============================] - 90s 3s/step - loss: 0.1302 - accuracy: 0.9679 - val_loss: 0.1347 - val_accuracy: 0.9682\n",
      "26/26 [==============================] - 92s 4s/step - loss: 0.1266 - accuracy: 0.9679 - val_loss: 0.1293 - val_accuracy: 0.9683\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "plot_model(model_bilstm_lstm)\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2cdda9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hi, My name is \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Aman Kharwal \n",
       " I\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " am from \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    India\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " </br> I want to work with Google </br> Steve Jobs is My Inspiration</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = nlp('Hi, My name is Aman Kharwal \\n I am from India \\n I want to work with Google \\n Steve Jobs is My Inspiration')\n",
    "displacy.render(text, style = 'ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77349f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
